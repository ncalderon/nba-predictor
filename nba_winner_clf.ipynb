{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NBA Winner Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.set_option(\"display.max_colwidth\", None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "import model.train as train\n",
    "import model.config as model_config\n",
    "import utils\n",
    "import model.dataset.game_matchup as gm\n",
    "import utils_nba_winner_classifier as utils_exp\n",
    "import qgrid\n",
    "from yellowbrick import classifier, features\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"husl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_qgrid(df_):\n",
    "    qgrid_widget = qgrid.show_grid(df_, show_toolbar=True , grid_options={'forceFitColumns': False, 'defaultColumnWidth': 200})\n",
    "    qgrid_widget"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_results(experiment_name, results, figsize=(20,10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    results_df = utils.map_results_to_df(results)\n",
    "    a = sns.pointplot(data=results_df,\n",
    "              kind=\"point\", x=\"season_test\", y=\"balanced_accuracy\", hue=\"model\"\n",
    "              )\n",
    "    a.set_title(\n",
    "        f\"{experiment_name}-balanced_accuracy\")\n",
    "    a.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_results_df():\n",
    "    return utils_exp.exp_results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = ['precision', 'balanced_accuracy', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "def print_prettier_exp_results(exp_name, metric='roc_auc'):\n",
    "    ba_df = get_results_df()[['exp_name', 'model',\n",
    "       f'{metric}_mean', f'{metric}_std']]\n",
    "    return ba_df[ba_df.exp_name == exp_name].sort_values(by=[f\"{metric}_mean\"], ascending=False)\n",
    "\n",
    "\n",
    "reg_metrics = ['mae', 'mse', 'rmse']\n",
    "\n",
    "def print_prettier_reg_exp_results(exp_name, metric='mse'):\n",
    "    ba_df = get_results_df('reg')\n",
    "    return ba_df[ba_df.exp_name == exp_name].sort_values(by=[f\"{metric}_mean\"], ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gm_df = gm.load_game_matchup_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df = gm_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df[\"WIN\"] = [\"HOME\" if x == 1 else \"VISITOR\" for x in eda_df['HOME_TEAM_WINS']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df[\"WIN\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pandas Profiling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "profile = ProfileReport(eda_df, title='Pandas Profiling Report', pool_size=4,\n",
    "                        minimal=True,\n",
    "                        explorative=True,\n",
    "                           correlations={\n",
    "             \"pearson\": {\"calculate\": True},\n",
    "             \"spearman\": {\"calculate\": True},\n",
    "             \"kendall\": {\"calculate\": True},\n",
    "             \"phi_k\": {\"calculate\": False},\n",
    "             \"cramers\": {\"calculate\": False},\n",
    "         })\n",
    "profile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### By Team"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "last_season = eda_df[eda_df.SEASON == 2018]\n",
    "last_season.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "home_wins_df = last_season[[\"HOME_TEAM_NAME\", \"HOME_TEAM_WINS\"]].groupby(by=\"HOME_TEAM_NAME\") \\\n",
    ".agg({'HOME_TEAM_NAME':'count', 'HOME_TEAM_WINS': 'sum'}) \\\n",
    ".sort_values(by=[\"HOME_TEAM_WINS\"], ascending=False)\n",
    "home_wins_df[\"TEAM_LOSS\"] = home_wins_df[\"HOME_TEAM_NAME\"] - home_wins_df[\"HOME_TEAM_WINS\"]\n",
    "home_wins_df.drop(labels=[\"HOME_TEAM_NAME\"], axis=1, inplace=True)\n",
    "home_wins_df.rename(columns={\"HOME_TEAM_WINS\": \"TEAM_WINS\"}, inplace=True)\n",
    "home_wins_df.index.rename(\"TEAM_NAME\", inplace=True)\n",
    "home_wins_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visitor_wins_df = last_season[[\"VISITOR_TEAM_NAME\", \"HOME_TEAM_WINS\"]].groupby(by=\"VISITOR_TEAM_NAME\") \\\n",
    ".agg({'VISITOR_TEAM_NAME':'count', 'HOME_TEAM_WINS': 'sum'}) \\\n",
    ".sort_values(by=[\"HOME_TEAM_WINS\"], ascending=False)\n",
    "visitor_wins_df[\"TEAM_WINS\"] = visitor_wins_df[\"VISITOR_TEAM_NAME\"] - visitor_wins_df[\"HOME_TEAM_WINS\"]\n",
    "visitor_wins_df.rename(columns={\"HOME_TEAM_WINS\": \"TEAM_LOSS\"}, inplace=True)\n",
    "visitor_wins_df.drop(labels=[\"VISITOR_TEAM_NAME\"], axis=1, inplace=True)\n",
    "visitor_wins_df.index.rename(\"TEAM_NAME\", inplace=True)\n",
    "#visitor_wins_df = visitor_wins_df[[\"TEAM_WINS\"]]\n",
    "visitor_wins_df = visitor_wins_df[[\"TEAM_WINS\", \"TEAM_LOSS\"]]\n",
    "visitor_wins_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "home_wins_df.combine(visitor_wins_df, lambda s1, s2: s1 + s2).sort_values(by=\"TEAM_WINS\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mil_df = last_season[(last_season.VISITOR_TEAM_NAME == 'MIL') | (last_season.HOME_TEAM_NAME == 'MIL')]\n",
    "mil_df['GAME_N'] = range(1, 83,1)\n",
    "mil_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sns.regplot(x=mil_df[\"GAME_N\"], y=mil_df[\"GAME_N\"])\n",
    "#sns.catplot(data=mil_df[[\"HOME_TEAM_NAME\", \"GAME_N\", \"WIN\"]],  x=\"GAME_N\", y=\"HOME_TEAM_NAME\", kind=\"bar\", hue=\"WIN\", palette=\"Set2\")\n",
    "#sns.displot(data=mil_df, x=\"GAME_N\", hue=\"WIN\", multiple=\"stack\")\n",
    "#plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scatter plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gm_df.groupby(by=\"SEASON\").count()[\"GAME_DATE_EST\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos observar que no todas las temporadas tiene la misma cantidad de partidos. Esto es debido a la siguientes razones:\n",
    "\n",
    "- 2011: Los jugadores hicieron una huelga debido a no estar de acuerdo con los salarios de los mismos y el limite salarial de las franquicias.\n",
    "- 2012: Un partido entre el equipo de Boston e Indiana fue suspedindo el cual despues no fue reprogramado, y al final de la temporada se decidio ya no reprogramarlo debido a que la clasificacion a playoff ya estaba decidida y no afectaba el resultado.\n",
    "\n",
    "Por tanto se seleccionaran solo las temporadas a partir del 2013(inclusive)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = gm_df[gm_df.SEASON >= 2013]\n",
    "seasons_size = len(df.SEASON.unique())\n",
    "seasons = list(df.SEASON.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_prefix = \"\"\n",
    "exp_group_name = \"experiments\"\n",
    "results_total = []\n",
    "utils_exp.exp_results = []\n",
    "TARGET = \"HOME_TEAM_WINS\"\n",
    "exp_X_columns = model_config.X_COLUMNS\n",
    "exp_y_columns = [TARGET]\n",
    "\n",
    "\n",
    "models = utils_exp.get_clf_models()\n",
    "\n",
    "sscv = utils.SeasonSeriesSplit(df)\n",
    "df_sscv = sscv.get_df()\n",
    "X = df_sscv[exp_X_columns]\n",
    "y = df_sscv['HOME_TEAM_WINS']\n",
    "\n",
    "#utils_exp.exp_results = utils.deserialize_object(exp_group_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment using 1 season"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}1_season\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=1, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = utils_exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica auc_roc se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***XGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***LGB***: Obtuvo el tercer promedio mas alto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Visualize experiments results\n",
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de los resultados del experimento se observa lo siguiente:\n",
    "\n",
    "    - SVM supero con gran ventaja a los demas algoritmos. El algoritmo XGB fue el que mas cerca estuvo a SVM.\n",
    "    - Se evidencian los siguientes patrones:\n",
    "        - Para todos los algoritmos hubo una notable mejoria al ser entrenados con la temporada 2014 para predecir la temporada 2015.\n",
    "        - Sin embargo en las siguientes validaciones para la temporada 2016 y 2017 nuevamente se evidencia una caida en el rendimiento de todos los algoritmos.\n",
    "        - Para predecir la temporada 2018 pues este rendimiento mejoro."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment using StandardScaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}scaled_data\"\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('numerical', num_pipeline, model_config.X_NUM_COLS)\n",
    "    ], remainder='passthrough')\n",
    "#transformed_data = preprocessor.fit_transform(df)\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=1, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y, preprocessor)\n",
    "names, results = utils_exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***XGB***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***RF***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***LGB***: Obtuvo el tercer promedio mas alto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de los resultados del experimento se observa lo siguiente:\n",
    "\n",
    "    - A pesar de que obtuvo el mejor rendimiento en la primera validacion, el algoritmo SVM empeoro en cada validacion. Cabe se√±alar tambien que ademas de que cayo empicado arrojo un rendimiento de un 50% para la ultima validacion.\n",
    "    - Para los demas algoritmos se evidencia un patron en la validacion de predecir al temporada 15 donde para todos fue un mejor rendimiento que para predecir la temporada 14.\n",
    "    - Para la temporada 16 el rendimiento bajo para todos.\n",
    "    - Para la temporada 17 el algoritmo KNN presenta una notable mejoria por enciman de los demas algoritmos.\n",
    "    - Para la ultima temporada el algoritmo XGB fue el que obtuvo el mejor rendimiento."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment: train: 2s; test: 1s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}2_seasons\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=2, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = utils_exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***LGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***XGB***: Obtuvo el tercer promedio mas alto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de los resultados del experimento se observa lo siguiente:\n",
    "\n",
    "    - Claramente SVM fue el algoritmo que mejor rendimiento tuvo en cada una de las validaciones.\n",
    "    - Se evidencia un patron en el que todos los algoritmos tuvieron una caida de su rendimiento en la temporada 16 y continuo cayendo en el 17, a excepcion de RF que mejoro.\n",
    "    - Otro patron que se evidencia es que para la temporada 2018 todos los algoritmos mejoraron.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment: train: 3s; test: 1s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}3_seasons\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=3, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = utils_exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***LGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***XGB***: Obtuvo el tercer promedio mas alto.\n",
    "\n",
    " Se destaca tambien que estos resultados son muy similares a los del experimento anterior(2s 1s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - Nuevamente SVM fue el algoritmo que mejor rendimiento tuvo en cada una de las validaciones.\n",
    "    - Se evidencia un ligero patron de ascenso para los tres algoritmos con mejor rendimiento, lo cual indica cierta estabilidad en las validaciones de este experimento comparada con los experimentos anteriores.\n",
    "    - Tambien se evidencia que comparado con los experimentos anteriores, en este el peor rendimiento estuvo en la primera validacion la del 16."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment train: 3q ; test: 1q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}3q_1q\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.quarter_split(train_size=3, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = utils_exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***LGB***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***KNN***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***SVM***: Obtuvo el tercer promedio mas alto.\n",
    "\n",
    " Comparado con el anterior experimento(3s 1s), estos resultados son peores, pero es importante destacar que se hicieron muchisimas mas validaciones que los experimentos anteriores. Ver grafico debajo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - SVM fue bastante inestable, tuvo casi igual de picos de buenos rendimientos como caidas de malos rendientos.\n",
    "    - Se observa un patron, en cada cuarto de temporada 0.25, que corresponde a la mitad(desde 0.25 a 0.5) pues el rendimiento de cada algoritmo tuvo una caida."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment train: 4q ; test: 2q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}4q_2q\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.quarter_split(train_size=4, test_size=2)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = utils_exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***XGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***RF***: Obtuvo el tercer promedio mas alto.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - SVM fue un poco mas estable que el experimento anterior.\n",
    "    - Se observa un patron, en cada validacion 0.5-0.75, es decir, que se trata de predecir la segunda mitad de la temporada, usando la primera mitad de la temporada actual y la ultima mitad de la anterior, pues hay una caida en el rendimiento de los algoritmos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment remove first 1q ; train: 2q ; test: 1q\n",
    "\n",
    "En este experimento se eliminara el primer cuarto de cada temporada, y se entrenara el modelo con los 2 siguientes cuartos, y como test 1 cuarto de temporada. La ventana deslisante sera de 1 cuarto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}r1q_2q_1q\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.quarter_split(train_size=2, test_size=1, skip=[0.25])\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = utils_exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***KNN***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***LGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***SVM***: Obtuvo el tercer promedio mas alto.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Visualize experiments results\n",
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - Se observa un patron, en cada validacion .75, es decir, que se trata de predecir el ultimo cuarto de temporada, pues hay una caida del rendimiento en el caso del algoritmo SVM, sin embargo no es lo mismo para los otros algoritmos, al menos no para todos las validaciones .75."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiments comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.serialize_object(exp_group_name, utils_exp.exp_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_df = get_results_df()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ba_df = exp_df[['exp_name', 'model',\n",
    "       'balanced_accuracy_mean', 'balanced_accuracy_std']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#show_qgrid(exp_df)\n",
    "qgrid_widget = qgrid.show_grid(ba_df, show_toolbar=True , grid_options={'forceFitColumns': False, 'defaultColumnWidth': 200})\n",
    "qgrid_widget"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ba_df.sort_values(\n",
    "    by=[\"balanced_accuracy_mean\"]\n",
    "    , ascending=False)[:6].reset_index().drop(labels=[\"index\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basado en la metrica 'balanced_accuracy_mean' se observa:\n",
    "\n",
    "- El algoritmo SVM obtuvo el mejor rendimiento para la mayoria de los experimentos. LGB fue el algoritmo siguiente que obtuvo el mejor rendimiento.\n",
    "- En los experimentos que se obtuvo el mejor rendimiento fueron usando 2 y 3 temporadas para entrenamiento para predecir al siguiente.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot experiment results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.serialize_object(\"results\", utils_exp.exp_results)\n",
    "utils.serialize_object(\"results_total\", results_total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Balance Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"balanced_accuracy\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"precision\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"recall\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### F1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"f1\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ROC AUC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"roc_auc\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}