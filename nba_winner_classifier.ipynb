{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NBA Winner Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#pd.options.display.max_columns = None\n",
    "#pd.set_option(\"display.max_colwidth\", None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "#pd.set_option(\"display.max_rows\", None)\n",
    "import model.train as train\n",
    "import model.config as model_config\n",
    "import utils\n",
    "import model.dataset.game_matchup as gm\n",
    "import experiments as exp\n",
    "import qgrid\n",
    "from yellowbrick import classifier, features, regressor\n",
    "import yellowbrick.model_selection as ms\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set(style=\"whitegrid\", palette=\"husl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_qgrid(df_):\n",
    "    qgrid_widget = qgrid.show_grid(df_, show_toolbar=True , grid_options={'forceFitColumns': False, 'defaultColumnWidth': 200})\n",
    "    qgrid_widget"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_results(experiment_name, results, figsize=(20,10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    results_df = exp.map_results_to_df(results)\n",
    "    a = sns.pointplot(data=results_df,\n",
    "              kind=\"point\", x=\"season_test\", y=\"balanced_accuracy\", hue=\"model\"\n",
    "              )\n",
    "    a.set_title(\n",
    "        f\"{experiment_name}-balanced_accuracy\")\n",
    "    a.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_results_df(algorithm_type='clf'):\n",
    "    if algorithm_type == 'reg':\n",
    "        return pd.DataFrame(exp.reg_exp_results)\n",
    "    else:\n",
    "        return pd.DataFrame(exp.exp_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metrics = ['precision', 'balanced_accuracy', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "def print_prettier_exp_results(exp_name, metric='roc_auc'):\n",
    "    ba_df = get_results_df()[['exp_name', 'model',\n",
    "       f'{metric}_mean', f'{metric}_std']]\n",
    "    return ba_df[ba_df.exp_name == exp_name].sort_values(by=[f\"{metric}_mean\"], ascending=False)\n",
    "\n",
    "\n",
    "reg_metrics = ['mae', 'mse', 'rmse']\n",
    "\n",
    "def print_prettier_reg_exp_results(exp_name, metric='mse'):\n",
    "    ba_df = get_results_df('reg')\n",
    "    return ba_df[ba_df.exp_name == exp_name].sort_values(by=[f\"{metric}_mean\"], ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gm_df = gm.load_game_matchup_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df = gm_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df[\"WIN\"] = [\"HOME\" if x == 1 else \"VISITOR\" for x in eda_df['HOME_TEAM_WINS']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "eda_df[\"WIN\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pandas Profiling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "profile = ProfileReport(eda_df, title='Pandas Profiling Report', pool_size=4,\n",
    "                        minimal=True,\n",
    "                        explorative=True,\n",
    "                           correlations={\n",
    "             \"pearson\": {\"calculate\": True},\n",
    "             \"spearman\": {\"calculate\": True},\n",
    "             \"kendall\": {\"calculate\": True},\n",
    "             \"phi_k\": {\"calculate\": False},\n",
    "             \"cramers\": {\"calculate\": False},\n",
    "         })\n",
    "profile"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### By Team"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "last_season = eda_df[eda_df.SEASON == 2018]\n",
    "last_season.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "home_wins_df = last_season[[\"HOME_TEAM_NAME\", \"HOME_TEAM_WINS\"]].groupby(by=\"HOME_TEAM_NAME\") \\\n",
    ".agg({'HOME_TEAM_NAME':'count', 'HOME_TEAM_WINS': 'sum'}) \\\n",
    ".sort_values(by=[\"HOME_TEAM_WINS\"], ascending=False)\n",
    "home_wins_df[\"TEAM_LOSS\"] = home_wins_df[\"HOME_TEAM_NAME\"] - home_wins_df[\"HOME_TEAM_WINS\"]\n",
    "home_wins_df.drop(labels=[\"HOME_TEAM_NAME\"], axis=1, inplace=True)\n",
    "home_wins_df.rename(columns={\"HOME_TEAM_WINS\": \"TEAM_WINS\"}, inplace=True)\n",
    "home_wins_df.index.rename(\"TEAM_NAME\", inplace=True)\n",
    "home_wins_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visitor_wins_df = last_season[[\"VISITOR_TEAM_NAME\", \"HOME_TEAM_WINS\"]].groupby(by=\"VISITOR_TEAM_NAME\") \\\n",
    ".agg({'VISITOR_TEAM_NAME':'count', 'HOME_TEAM_WINS': 'sum'}) \\\n",
    ".sort_values(by=[\"HOME_TEAM_WINS\"], ascending=False)\n",
    "visitor_wins_df[\"TEAM_WINS\"] = visitor_wins_df[\"VISITOR_TEAM_NAME\"] - visitor_wins_df[\"HOME_TEAM_WINS\"]\n",
    "visitor_wins_df.rename(columns={\"HOME_TEAM_WINS\": \"TEAM_LOSS\"}, inplace=True)\n",
    "visitor_wins_df.drop(labels=[\"VISITOR_TEAM_NAME\"], axis=1, inplace=True)\n",
    "visitor_wins_df.index.rename(\"TEAM_NAME\", inplace=True)\n",
    "#visitor_wins_df = visitor_wins_df[[\"TEAM_WINS\"]]\n",
    "visitor_wins_df = visitor_wins_df[[\"TEAM_WINS\", \"TEAM_LOSS\"]]\n",
    "visitor_wins_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "home_wins_df.combine(visitor_wins_df, lambda s1, s2: s1 + s2).sort_values(by=\"TEAM_WINS\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mil_df = last_season[(last_season.VISITOR_TEAM_NAME == 'MIL') | (last_season.HOME_TEAM_NAME == 'MIL')]\n",
    "mil_df['GAME_N'] = range(1, 83,1)\n",
    "mil_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sns.regplot(x=mil_df[\"GAME_N\"], y=mil_df[\"GAME_N\"])\n",
    "#sns.catplot(data=mil_df[[\"HOME_TEAM_NAME\", \"GAME_N\", \"WIN\"]],  x=\"GAME_N\", y=\"HOME_TEAM_NAME\", kind=\"bar\", hue=\"WIN\", palette=\"Set2\")\n",
    "#sns.displot(data=mil_df, x=\"GAME_N\", hue=\"WIN\", multiple=\"stack\")\n",
    "#plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scatter plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gm_df.groupby(by=\"SEASON\").count()[\"GAME_DATE_EST\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos observar que no todas las temporadas tiene la misma cantidad de partidos. Esto es debido a la siguientes razones:\n",
    "\n",
    "- 2011: Los jugadores hicieron una huelga debido a no estar de acuerdo con los salarios de los mismos y el limite salarial de las franquicias.\n",
    "- 2012: Un partido entre el equipo de Boston e Indiana fue suspedindo el cual despues no fue reprogramado, y al final de la temporada se decidio ya no reprogramarlo debido a que la clasificacion a playoff ya estaba decidida y no afectaba el resultado.\n",
    "\n",
    "Por tanto se seleccionaran solo las temporadas a partir del 2013(inclusive)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = gm_df[gm_df.SEASON >= 2013]\n",
    "seasons_size = len(df.SEASON.unique())\n",
    "seasons = list(df.SEASON.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_prefix = \"\"\n",
    "exp_group_name = \"experiments\"\n",
    "results_total = []\n",
    "exp.exp_results = []\n",
    "TARGET = \"HOME_TEAM_WINS\"\n",
    "exp_X_columns = model_config.X_COLUMNS\n",
    "exp_y_columns = [TARGET]\n",
    "\n",
    "\n",
    "models = exp.get_clf_models()\n",
    "\n",
    "sscv = utils.SeasonSeriesSplit(df)\n",
    "df_sscv = sscv.get_df()\n",
    "X = df_sscv[exp_X_columns]\n",
    "y = df_sscv['HOME_TEAM_WINS']\n",
    "\n",
    "#exp.exp_results = utils.deserialize_object(exp_group_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment using 1 season"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}1_season\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=1, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica auc_roc se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***XGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***LGB***: Obtuvo el tercer promedio mas alto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Visualize experiments results\n",
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de los resultados del experimento se observa lo siguiente:\n",
    "\n",
    "    - SVM supero con gran ventaja a los demas algoritmos. El algoritmo XGB fue el que mas cerca estuvo a SVM.\n",
    "    - Se evidencian los siguientes patrones:\n",
    "        - Para todos los algoritmos hubo una notable mejoria al ser entrenados con la temporada 2014 para predecir la temporada 2015.\n",
    "        - Sin embargo en las siguientes validaciones para la temporada 2016 y 2017 nuevamente se evidencia una caida en el rendimiento de todos los algoritmos.\n",
    "        - Para predecir la temporada 2018 pues este rendimiento mejoro."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment using StandardScaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}scaled_data\"\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('std_scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('numerical', num_pipeline, model_config.X_NUM_COLS)\n",
    "    ], remainder='passthrough')\n",
    "#transformed_data = preprocessor.fit_transform(df)\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=1, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y, preprocessor)\n",
    "names, results = exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***XGB***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***RF***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***LGB***: Obtuvo el tercer promedio mas alto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de los resultados del experimento se observa lo siguiente:\n",
    "\n",
    "    - A pesar de que obtuvo el mejor rendimiento en la primera validacion, el algoritmo SVM empeoro en cada validacion. Cabe señalar tambien que ademas de que cayo empicado arrojo un rendimiento de un 50% para la ultima validacion.\n",
    "    - Para los demas algoritmos se evidencia un patron en la validacion de predecir al temporada 15 donde para todos fue un mejor rendimiento que para predecir la temporada 14.\n",
    "    - Para la temporada 16 el rendimiento bajo para todos.\n",
    "    - Para la temporada 17 el algoritmo KNN presenta una notable mejoria por enciman de los demas algoritmos.\n",
    "    - Para la ultima temporada el algoritmo XGB fue el que obtuvo el mejor rendimiento."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment: train: 2s; test: 1s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}2_seasons\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=2, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***LGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***XGB***: Obtuvo el tercer promedio mas alto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de los resultados del experimento se observa lo siguiente:\n",
    "\n",
    "    - Claramente SVM fue el algoritmo que mejor rendimiento tuvo en cada una de las validaciones.\n",
    "    - Se evidencia un patron en el que todos los algoritmos tuvieron una caida de su rendimiento en la temporada 16 y continuo cayendo en el 17, a excepcion de RF que mejoro.\n",
    "    - Otro patron que se evidencia es que para la temporada 2018 todos los algoritmos mejoraron.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment: train: 3s; test: 1s"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}3_seasons\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=3, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***LGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***XGB***: Obtuvo el tercer promedio mas alto.\n",
    "\n",
    " Se destaca tambien que estos resultados son muy similares a los del experimento anterior(2s 1s)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - Nuevamente SVM fue el algoritmo que mejor rendimiento tuvo en cada una de las validaciones.\n",
    "    - Se evidencia un ligero patron de ascenso para los tres algoritmos con mejor rendimiento, lo cual indica cierta estabilidad en las validaciones de este experimento comparada con los experimentos anteriores.\n",
    "    - Tambien se evidencia que comparado con los experimentos anteriores, en este el peor rendimiento estuvo en la primera validacion la del 16."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment train: 3q ; test: 1q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}3q_1q\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.quarter_split(train_size=3, test_size=1)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***LGB***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***KNN***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***SVM***: Obtuvo el tercer promedio mas alto.\n",
    "\n",
    " Comparado con el anterior experimento(3s 1s), estos resultados son peores, pero es importante destacar que se hicieron muchisimas mas validaciones que los experimentos anteriores. Ver grafico debajo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - SVM fue bastante inestable, tuvo casi igual de picos de buenos rendimientos como caidas de malos rendientos.\n",
    "    - Se observa un patron, en cada cuarto de temporada 0.25, que corresponde a la mitad(desde 0.25 a 0.5) pues el rendimiento de cada algoritmo tuvo una caida."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment train: 4q ; test: 2q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}4q_2q\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.quarter_split(train_size=4, test_size=2)\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***SVM***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***XGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***RF***: Obtuvo el tercer promedio mas alto.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - SVM fue un poco mas estable que el experimento anterior.\n",
    "    - Se observa un patron, en cada validacion 0.5-0.75, es decir, que se trata de predecir la segunda mitad de la temporada, usando la primera mitad de la temporada actual y la ultima mitad de la anterior, pues hay una caida en el rendimiento de los algoritmos."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment remove first 1q ; train: 2q ; test: 1q\n",
    "\n",
    "En este experimento se eliminara el primer cuarto de cada temporada, y se entrenara el modelo con los 2 siguientes cuartos, y como test 1 cuarto de temporada. La ventana deslisante sera de 1 cuarto."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_name = f\"{exp_prefix}r1q_2q_1q\"\n",
    "\n",
    "folds, train_seasons, test_seasons = sscv.quarter_split(train_size=2, test_size=1, skip=[0.25])\n",
    "params = (experiment_name, models, folds, train_seasons, test_seasons, X, y)\n",
    "names, results = exp.run_experiment(*params)\n",
    "results_total.append((experiment_name, results))\n",
    "\n",
    "print_prettier_exp_results(experiment_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Respecto a la metrica balanced_accuracy se observa lo siguiente:\n",
    "\n",
    "    - ***KNN***: Fue el algoritmo que alcanzo el mejor promedio.\n",
    "    - ***LGB***: Obtuvo el segundo promedio mas alto.\n",
    "    - ***SVM***: Obtuvo el tercer promedio mas alto.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#### Visualize experiments results\n",
    "plot_results(experiment_name, results, figsize=(20,10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el grafico de cada validacion se observa lo siguiente:\n",
    "\n",
    "    - Se observa un patron, en cada validacion .75, es decir, que se trata de predecir el ultimo cuarto de temporada, pues hay una caida del rendimiento en el caso del algoritmo SVM, sin embargo no es lo mismo para los otros algoritmos, al menos no para todos las validaciones .75."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiments comparison"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.serialize_object(exp_group_name, exp.exp_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp_df = get_results_df()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ba_df = exp_df[['exp_name', 'model',\n",
    "       'balanced_accuracy_mean', 'balanced_accuracy_std']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#show_qgrid(exp_df)\n",
    "qgrid_widget = qgrid.show_grid(ba_df, show_toolbar=True , grid_options={'forceFitColumns': False, 'defaultColumnWidth': 200})\n",
    "qgrid_widget"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ba_df.sort_values(\n",
    "    by=[\"balanced_accuracy_mean\"]\n",
    "    , ascending=False)[:6].reset_index().drop(labels=[\"index\"], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basado en la metrica 'balanced_accuracy_mean' se observa:\n",
    "\n",
    "- El algoritmo SVM obtuvo el mejor rendimiento para la mayoria de los experimentos. LGB fue el algoritmo siguiente que obtuvo el mejor rendimiento.\n",
    "- En los experimentos que se obtuvo el mejor rendimiento fueron usando 2 y 3 temporadas para entrenamiento para predecir al siguiente.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Plot experiment results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "utils.serialize_object(\"results\", exp.exp_results)\n",
    "utils.serialize_object(\"results_total\", results_total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Balance Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"balanced_accuracy\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"precision\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Recall"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"recall\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### F1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"f1\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ROC AUC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "exp.plot_to_compare_experiments(\n",
    "    results_total,\n",
    "    metric=\"roc_auc\",\n",
    "    figsize=(25, 35),\n",
    "    use_pointplot=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tuning model\n",
    "\n",
    "A partir de los experimentos y basado en las observaciones realizadas trabajaremos de ahora en adelante solo con los modelos: SVM y LGB.\n",
    "\n",
    "Para el entrenamiento cada modelo utilizaremos las variantes:\n",
    "\n",
    "- Train: 2 seasons ; Test: 1 season ; Ventana deslizante de: 1 season\n",
    "- Train: 3 seasons ; Test: 1 season ; Ventana deslizante de: 1 season"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "results_total = []\n",
    "exp.exp_results = []\n",
    "experiment_name = f\"{exp_prefix}2_season_tunning\"\n",
    "best_models = [\n",
    "    (\"RF\", RandomForestClassifier(n_estimators=300,\n",
    "                                                max_depth=11,\n",
    "                                                n_jobs=-1,\n",
    "                                                random_state=0,\n",
    "                                                criterion='entropy',\n",
    "                                                max_features=19,\n",
    "                                                min_samples_leaf=9,\n",
    "                                 )),\n",
    "    ('SVM', SVC(kernel='linear', random_state=0,\n",
    "                              C=63.513891775842986,\n",
    "                              gamma=76.1465194934807,\n",
    "                              degree= 0.4300244876201068))\n",
    "]\n",
    "folds, train_seasons, test_seasons = sscv.split(train_size=2, test_size=1)\n",
    "X, y = train.X_y_values(df, exp_X_columns, exp_y_columns)\n",
    "#params = (experiment_name, best_models, folds, train_seasons, test_seasons, X, y)\n",
    "#names, results = exp.run_experiment(*params)\n",
    "#results_total.append((experiment_name, results))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from yellowbrick.style import set_palette\n",
    "import warnings\n",
    "import io\n",
    "from sklearn import base, metrics, model_selection, preprocessing, tree\n",
    "from yellowbrick.classifier import ROCAUC\n",
    "import yellowbrick.classifier\n",
    "\n",
    "set_palette('flatui')\n",
    "fold_last_season = folds[len(folds)-1:]\n",
    "name, model = best_models[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Explore models perfomance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "fold_last_season = folds[len(folds)-1:]\n",
    "for name, model in best_models:\n",
    "    for i, idx_data in enumerate(fold_last_season):\n",
    "        print(f\"Test season: {test_seasons[i]}\")\n",
    "        train_idx, test_idx = idx_data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx].ravel(), y[test_idx].ravel()\n",
    "        y_true = y_test\n",
    "        fit_info = model.fit(X_train, y_train)\n",
    "\n",
    "        #with sns.plotting_context('paper'):\n",
    "        #    fig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n",
    "        cm_viz = classifier.ConfusionMatrix(model, percent=True)\n",
    "        cm_viz.fit(X_train, y_train)\n",
    "        cm_viz.score(X_test, y_test)\n",
    "        cm_viz.show()\n",
    "        #cm_viz.poof()\n",
    "        #with sns.plotting_context('talk'):\n",
    "            #fig, ax = plt.subplots(figsize=(20, 20), dpi=300)\n",
    "        plt.figure(figsize=(20, 20), dpi=300)\n",
    "        fi_viz = features.FeatureImportances(model, labels=exp_X_columns, relative=False)\n",
    "        fi_viz.fit(X_train, y_train)\n",
    "        fi_viz.score(X_test, y_test)\n",
    "        #fi_viz.poof()\n",
    "        fi_viz.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ROC AUC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i, idx_data in enumerate(fold_last_season):\n",
    "    print(f\"Test season: {test_seasons[i]}\")\n",
    "    train_idx, test_idx = idx_data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx].ravel(), y[test_idx].ravel()\n",
    "    y_true = y_test\n",
    "    fit_info = model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    roc_auc_score(y_true, y_pred, average='weighted')\n",
    "    print(f'roc_auc: {roc_auc_score}')\n",
    "    roc_viz = ROCAUC(model, classes=['LOSS', 'WIN'])\n",
    "    roc_viz.score(X_test, y_test)\n",
    "    roc_viz.show()\n",
    "\n",
    "    roc_viz = classifier.ClassPredictionError(model, classes=['LOSS', 'WIN'])\n",
    "    roc_viz.score(X_test, y_test)\n",
    "    roc_viz.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Hyperopt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "v_folds = folds[:-1]\n",
    "test_fold = folds[-1]\n",
    "#name, model = best_models[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### SVM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hyperopt_cv(params):\n",
    "    cv_results = {\n",
    "            \"roc_auc\": []\n",
    "        }\n",
    "\n",
    "    #del params['normalize']\n",
    "    #del params['scale']\n",
    "\n",
    "    for train_idx, test_idx in v_folds:\n",
    "        X[train_idx], X[test_idx] = utils.feature_scaling(X[train_idx], X[test_idx], 5)\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx].ravel(), y[test_idx].ravel()\n",
    "        y_true = y_test\n",
    "        model = SVC(**params)\n",
    "        fit_info = model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred, average='weighted')\n",
    "        cv_results[\"roc_auc\"].append(roc_auc)\n",
    "\n",
    "    return np.mean(cv_results[\"roc_auc\"])\n",
    "\n",
    "space4svm = {\n",
    "    'C': hp.uniform('C', 0, 100),\n",
    "    'kernel': hp.choice('kernel', ['linear']),\n",
    "    'gamma': hp.uniform('gamma', 0, 100),\n",
    "    'degree': hp.uniform('degree', 0, 6)\n",
    "    #'scale': hp.choice('scale', [0, 1]),\n",
    "    #'normalize': hp.choice('normalize', [0, 1])\n",
    "}\n",
    "\n",
    "# best: {'C': 63.513891775842986, 'degree': 0.4300244876201068, 'gamma': 76.1465194934807, 'kernel': 0}\n",
    "def f(params):\n",
    "    acc = hyperopt_cv(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(f, space4svm, algo=tpe.suggest, max_evals=100, trials=trials)\n",
    "print(\"best:\", best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### RandomForest"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hyperopt_train_test(params):\n",
    "    cv_results = {\n",
    "            \"roc_auc\": []\n",
    "        }\n",
    "\n",
    "    #del params['normalize']\n",
    "    #del params['scale']\n",
    "\n",
    "    for train_idx, test_idx in v_folds:\n",
    "        X[train_idx], X[test_idx] = utils.feature_scaling(X[train_idx], X[test_idx], 5)\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx].ravel(), y[test_idx].ravel()\n",
    "        y_true = y_test\n",
    "        model = RandomForestClassifier(**params)\n",
    "        fit_info = model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred, average='weighted')\n",
    "        cv_results[\"roc_auc\"].append(roc_auc)\n",
    "\n",
    "    return np.mean(cv_results[\"roc_auc\"])\n",
    "\n",
    "space4rf = {\n",
    "    'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "    'max_features': hp.choice('max_features', range(1,20)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 500, 50)),\n",
    "    'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "    'min_samples_leaf':  hp.choice('min_samples_leaf',  np.arange(1, 20, step=1, dtype=int)),\n",
    "    #'min_samples_split': None,\n",
    "    #'max_leaf_nodes': None\n",
    "    #'scale': hp.choice('scale', [0, 1]),\n",
    "    #'normalize': hp.choice('normalize', [0, 1])\n",
    "}\n",
    "best = 0\n",
    "def f(params):\n",
    "    global best\n",
    "    acc = hyperopt_train_test(params)\n",
    "    if acc > best:\n",
    "        best = acc\n",
    "    print('new best:', best, params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(f, space4rf, algo=tpe.suggest, max_evals=300, trials=trials)\n",
    "print(\"best:\", best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgboost\n",
    "\n",
    "def hyperopt_train_test(params):\n",
    "    t = params['type']\n",
    "    del params['type']\n",
    "    if t == 'RF':\n",
    "        clf = RandomForestClassifier(**params)\n",
    "    elif t == 'SVM':\n",
    "        clf = SVC(**params)\n",
    "    elif t == 'XGB':\n",
    "        clf = xgb.XGBClassifier(**params)\n",
    "    elif t == 'LGB':\n",
    "        clf = lgb.LGBMClassifier(**params)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    cv_results = {\n",
    "            \"roc_auc\": []\n",
    "        }\n",
    "\n",
    "    #del params['normalize']\n",
    "    #del params['scale']\n",
    "\n",
    "    for train_idx, test_idx in v_folds:\n",
    "        X[train_idx], X[test_idx] = utils.feature_scaling(X[train_idx], X[test_idx], 5)\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx].ravel(), y[test_idx].ravel()\n",
    "        y_true = y_test\n",
    "        fit_info = clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        roc_auc = roc_auc_score(y_true, y_pred, average='weighted')\n",
    "        cv_results[\"roc_auc\"].append(roc_auc)\n",
    "\n",
    "    return np.mean(cv_results[\"roc_auc\"])\n",
    "\n",
    "space = hp.choice('classifier_type', [\n",
    "    {\n",
    "        'type': 'LGB',\n",
    "        'n_estimators': hp.choice('n_estimators2', range(100, 500, 50)),\n",
    "        'max_depth': hp.choice('max_depth2', range(1,20)),\n",
    "        'num_leaves': hp.choice('num_leaves2', np.arange( 30, 150, 1, dtype=int)),\n",
    "        'reg_alpha': hp.quniform('reg_alpha2', 0.0, 1.0, 0.1),\n",
    "        'reg_lambda': hp.quniform('reg_lambda2', 0.0, 1.0, 0.1),\n",
    "        'learning_rate': hp.loguniform('learning_rate2', np.log(0.01), np.log(0.2)),\n",
    "        'min_child_weight': hp.choice('min_child_weight2', [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]),\n",
    "        'min_child_samples': hp.choice('min_child_samples2', np.arange( 20, 500, 5, dtype=int))\n",
    "    },\n",
    "    {\n",
    "        'type': 'SVM',\n",
    "        'C': hp.uniform('C', 0, 100),\n",
    "        'kernel': hp.choice('kernel', ['linear']),\n",
    "        'gamma': hp.uniform('gamma', 0, 100),\n",
    "        'degree': hp.uniform('degree', 0, 6)\n",
    "    },{\n",
    "        'type': 'XGB',\n",
    "        'n_estimators': hp.choice('n_estimators1', range(100, 500, 50)),\n",
    "        'max_depth': hp.choice('max_depth1', range(1,20)),\n",
    "        #'num_leaves': hp.choice('num_leaves1', np.arange( 30, 150, 1, dtype=int)),\n",
    "        'reg_alpha': hp.quniform('reg_alpha1', 0.0, 1.0, 0.1),\n",
    "        'reg_lambda': hp.quniform('reg_lambda1', 0.0, 1.0, 0.1),\n",
    "        'learning_rate': hp.loguniform('learning_rate1', np.log(0.01), np.log(0.2)),\n",
    "        'min_child_weight': hp.choice('min_child_weight1', [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4]),\n",
    "        #'min_child_samples': hp.choice('min_child_samples1', np.arange( 20, 500, 5, dtype=int)),\n",
    "    },\n",
    "#     {\n",
    "#         'type': 'RF',\n",
    "#         'max_depth': hp.choice('max_depth', range(1,20)),\n",
    "#         'max_features': hp.choice('max_features', range(1,20)),\n",
    "#         'n_estimators': hp.choice('n_estimators', range(100, 500, 50)),\n",
    "#         'criterion': hp.choice('criterion', [\"gini\", \"entropy\"]),\n",
    "#         'min_samples_leaf':  hp.choice('min_samples_leaf',  np.arange(1, 20, step=1, dtype=int))\n",
    "#         #'scale': hp.choice('scale', [0, 1]),\n",
    "#         #'normalize': hp.choice('normalize', [0, 1])\n",
    "#     }\n",
    "])\n",
    "count = 0\n",
    "best = 0\n",
    "def f(params):\n",
    "    global best, count\n",
    "    count += 1\n",
    "    acc = hyperopt_train_test(params.copy())\n",
    "    if acc > best:\n",
    "        print('new best:', acc, 'using', params['type'])\n",
    "        best = acc\n",
    "    if count % 50 == 0:\n",
    "        print('iters:', count, ', acc:', acc, 'using', params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "trials = Trials()\n",
    "best = fmin(f, space, algo=tpe.suggest, max_evals=1000, trials=trials)\n",
    "print('best:', best)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}